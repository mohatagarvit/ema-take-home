{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NOTE: This is ONLY necessary in jupyter notebook.\n",
    "# Details: Jupyter runs an event-loop behind the scenes. \n",
    "#          This results in nested event-loops when we start an event-loop to make async queries.\n",
    "#          This is normally not allowed, we use nest_asyncio to allow it for convenience.  \n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
      "NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "from llama_index import (\n",
    "    GPTVectorStoreIndex,\n",
    "    GPTListIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    ServiceContext,\n",
    "    StorageContext\n",
    ")\n",
    "from llama_index.data_structs import Node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "We first show how to convert a Document into a set of Nodes, and insert into a DocumentStore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load documents\n",
    "# documents = SimpleDirectoryReader('../paul_graham_essay/data').load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "from llama_index import download_loader, GPTVectorStoreIndex, ServiceContext, StorageContext, load_index_from_storage\n",
    "from pathlib import Path\n",
    "# from utils import *\n",
    "from langchain.agents import Tool, initialize_agent\n",
    "\n",
    "urls = [\n",
    "    \"https://stanford-cs324.github.io/winter2022/lectures/introduction/\",\n",
    "    \"https://stanford-cs324.github.io/winter2022/lectures/harms-1/\",\n",
    "    \"https://stanford-cs324.github.io/winter2022/lectures/harms-2/\",\n",
    "    \"https://stanford-cs324.github.io/winter2022/lectures/capabilities/\",\n",
    "]\n",
    "table_urls = [\"https://github.com/Hannibal046/Awesome-LLM#milestone-papers\"]\n",
    "\n",
    "# UnstructuredURLLoader = download_loader(\"UnstructuredURLLoader\")\n",
    "# loader = UnstructuredURLLoader(urls=urls, continue_on_failure=False, headers={\"User-Agent\": \"value\"})\n",
    "# print(loader.load())\n",
    "\n",
    "BeautifulSoupWebReader = download_loader(\"BeautifulSoupWebReader\")\n",
    "loader = BeautifulSoupWebReader()\n",
    "documents = loader.load_data(urls=urls)\n",
    "\n",
    "from llama_index import GPTTreeIndex, SimpleDirectoryReader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support for other data_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize service context (set chunk size)\n",
    "service_context = ServiceContext.from_defaults(chunk_size_limit=1024)\n",
    "nodes = service_context.node_parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize storage context (by default it's in-memory)\n",
    "storage_context = StorageContext.from_defaults()\n",
    "storage_context.docstore.add_documents(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define List Index and Vector Index over Same Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 24846 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 24846 tokens\n"
     ]
    }
   ],
   "source": [
    "list_index = GPTListIndex(nodes, storage_context=storage_context)\n",
    "vector_index = GPTVectorStoreIndex(nodes, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Node/Query Engine for these Indices\n",
    "\n",
    "We define a Node and Query Engine for each Index. We then define an outer \"tool\" index to store\n",
    "these Nodes, which can be treated as metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_index_node = Node(\n",
    "    \"Lecture notes in Introduction.\",\n",
    "    doc_id=\"list_index\"\n",
    ")\n",
    "list_query_engine = list_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\", use_async=True\n",
    ")\n",
    "vector_index_node = Node(\n",
    "    \"Useful for questions around the author's education, from Paul Graham essay on What I Worked On.\",\n",
    "    doc_id=\"vector_index\"\n",
    ")\n",
    "vector_query_engine = vector_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\", use_async=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Vector Index Retriever for these Nodes\n",
    "\n",
    "Define a vector index on top of these Nodes which in turn correspond to the underlying query engines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 40 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 40 tokens\n"
     ]
    }
   ],
   "source": [
    "# create an outer \"tool\" index to store the underlying index information\n",
    "tool_index = GPTVectorStoreIndex([list_index_node, vector_index_node])\n",
    "# get retriever\n",
    "tool_retriever = tool_index.as_retriever(similarity_top_k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Router Query Engine\n",
    "\n",
    "We define a router query engine using the vector index retriever as input. This retriever will be used to retrieve \"Nodes\" which contain metadata for query engines. We also take as input a function that maps a Node to a query engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def node_to_query_engine(node: Node):\n",
    "    \"\"\"Convert node to query engine.\"\"\"\n",
    "    # NOTE: hardcode mapping in this case\n",
    "    mapping = {\n",
    "        \"list_index\": list_query_engine,\n",
    "        \"vector_index\": vector_query_engine\n",
    "    }\n",
    "    return mapping[node.get_doc_id()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.query_engine.router_query_engine import RetrieverRouterQueryEngine\n",
    "\n",
    "\n",
    "query_engine = RetrieverRouterQueryEngine(\n",
    "    tool_retriever,\n",
    "    node_to_query_engine\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 8 tokens\n",
      "> [retrieve] Total embedding token usage: 8 tokens\n",
      "INFO:llama_index.indices.common_tree.base:> Building index from nodes: 7 chunks\n",
      "> Building index from nodes: 7 chunks\n",
      "INFO:openai:message='OpenAI API response' path=https://api.openai.com/v1/completions processing_ms=1501 request_id=9e8d6ea8f052c41efcf6fcf32ff55580 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/completions processing_ms=1501 request_id=9e8d6ea8f052c41efcf6fcf32ff55580 response_code=200\n",
      "INFO:openai:message='OpenAI API response' path=https://api.openai.com/v1/completions processing_ms=1758 request_id=a56b3f1c3adfe30101911e3e7e3b49e0 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/completions processing_ms=1758 request_id=a56b3f1c3adfe30101911e3e7e3b49e0 response_code=200\n",
      "INFO:openai:message='OpenAI API response' path=https://api.openai.com/v1/completions processing_ms=1806 request_id=92f79d03b63ff38c2b9b2499d772d864 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/completions processing_ms=1806 request_id=92f79d03b63ff38c2b9b2499d772d864 response_code=200\n",
      "INFO:openai:message='OpenAI API response' path=https://api.openai.com/v1/completions processing_ms=1844 request_id=faac7a3dcba6bb22dc87c3503ebd0d66 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/completions processing_ms=1844 request_id=faac7a3dcba6bb22dc87c3503ebd0d66 response_code=200\n",
      "INFO:openai:message='OpenAI API response' path=https://api.openai.com/v1/completions processing_ms=2095 request_id=37d3c4510a31b27d42b2a8114ab440f1 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/completions processing_ms=2095 request_id=37d3c4510a31b27d42b2a8114ab440f1 response_code=200\n",
      "INFO:openai:message='OpenAI API response' path=https://api.openai.com/v1/completions processing_ms=2209 request_id=edb4226b83fc75f90a6f54132d474025 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/completions processing_ms=2209 request_id=edb4226b83fc75f90a6f54132d474025 response_code=200\n",
      "INFO:openai:message='OpenAI API response' path=https://api.openai.com/v1/completions processing_ms=2441 request_id=bd788bfbfc4a119b42bc2f460fb35098 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/completions processing_ms=2441 request_id=bd788bfbfc4a119b42bc2f460fb35098 response_code=200\n",
      "INFO:openai:message='OpenAI API response' path=https://api.openai.com/v1/completions processing_ms=2489 request_id=582e7fe8e084ee5df31efaeda47c298a response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/completions processing_ms=2489 request_id=582e7fe8e084ee5df31efaeda47c298a response_code=200\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 201 tokens\n",
      "> [get_response] Total LLM token usage: 201 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 26879 tokens\n",
      "> [get_response] Total LLM token usage: 26879 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "q = 'Which models did Google release in Oct 2018'\n",
    "response = query_engine.query(q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Google released the BERT (Bidirectional Encoder Representations from Transformers) model and the Perspective API in October 2018.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'> Source (Doc id: 48d7e737-91ef-495c-b521-578cfa7297ff): URL: https://stanford-cs324.github.io/winter2022/lectures/introduction/\\n\\n  Introduction | CS324  ...\\n\\n> Source (Doc id: 10e531be-5f00-4661-bd6c-f3442c4c50bf): URL: https://stanford-cs324.github.io/winter2022/lectures/introduction/\\n\\n\\\\(p\\\\). In practice, we d...\\n\\n> Source (Doc id: 116a265b-3f5f-4237-ba86-fb50106a08bb): URL: https://stanford-cs324.github.io/winter2022/lectures/introduction/\\n\\n= 0.4, \\\\quad\\\\quad\\\\quad p...\\n\\n> Source (Doc id: 96e02cf9-2239-44eb-835e-f0929568395c): URL: https://stanford-cs324.github.io/winter2022/lectures/introduction/\\n\\nrepresent an element \\\\(x...\\n\\n> Source (Doc id: 157f6610-6790-4161-aadb-b39011d58bbf): URL: https://stanford-cs324.github.io/winter2022/lectures/introduction/\\n\\n\\\\underbrace{p(\\\\text{spee...\\n\\n> Source (Doc id: c1270b0b-e6fe-4c7e-9073-68889202dfdb): URL: https://stanford-cs324.github.io/winter2022/lectures/introduction/\\n\\nvalues of \\\\(n\\\\).Now, the...\\n\\n> Source (Doc id: ff89a29b-2a04-4740-b2b2-09df4cf7b801): URL: https://stanford-cs324.github.io/winter2022/lectures/introduction/\\n\\nand RoBERTa.  Capabiliti...\\n\\n> Source (Doc id: 0b455021-a778-4cec-874d-ee04fd77aebf): URL: https://stanford-cs324.github.io/winter2022/lectures/introduction/\\n\\nsupervised learning. In ...\\n\\n> Source (Doc id: 58bbc077-ae7e-4fbe-915e-1caba0ffef70): URL: https://stanford-cs324.github.io/winter2022/lectures/introduction/\\n\\nfinished the program. He...\\n\\n> Source (Doc id: f5fccc85-2c2e-4708-a844-1f65cf9375e0): URL: https://stanford-cs324.github.io/winter2022/lectures/introduction/\\n\\nis access. Whereas small...\\n\\n> Source (Doc id: bd1d4e2e-46c2-468e-b644-29b912541004): URL: https://stanford-cs324.github.io/winter2022/lectures/introduction/\\n\\nTrevor Gale, Lauren E. G...\\n\\n> Source (Doc id: 5498d952-8fe0-47e9-95e4-b9a21aa7fd5c): URL: https://stanford-cs324.github.io/winter2022/lectures/harms-1/\\n\\n  Harms I | CS324     Link   ...\\n\\n> Source (Doc id: cede1c03-a728-4b8e-8459-4de2f3f85eae): URL: https://stanford-cs324.github.io/winter2022/lectures/harms-1/\\n\\nothers.For example, automatic...\\n\\n> Source (Doc id: 581ce143-5968-40af-8145-f6a68623c22c): URL: https://stanford-cs324.github.io/winter2022/lectures/harms-1/\\n\\nare historically discriminate...\\n\\n> Source (Doc id: d06a0301-f8b0-458f-b678-63d4d477270e): URL: https://stanford-cs324.github.io/winter2022/lectures/harms-1/\\n\\nmentions 21 definitions). Unf...\\n\\n> Source (Doc id: 4cd5d554-3e35-4ffa-97bc-574768af1795): URL: https://stanford-cs324.github.io/winter2022/lectures/harms-2/\\n\\n  Harms II | CS324     Link  ...\\n\\n> Source (Doc id: 93cc9542-88e9-49e2-9553-e9b8af2da5a4): URL: https://stanford-cs324.github.io/winter2022/lectures/harms-2/\\n\\nlecture, we will discuss two ...\\n\\n> Source (Doc id: 6fb991cc-ed59-4e8a-9ceb-7cd3339b6b16): URL: https://stanford-cs324.github.io/winter2022/lectures/harms-2/\\n\\nand bigotry comes from your p...\\n\\n> Source (Doc id: 40aea269-5248-4354-abbe-9751f478d2b8): URL: https://stanford-cs324.github.io/winter2022/lectures/harms-2/\\n\\nsentences from each toxicity ...\\n\\n> Source (Doc id: 11e73332-534a-46ab-b72a-de5f2bdf5141): URL: https://stanford-cs324.github.io/winter2022/lectures/harms-2/\\n\\nactors enlists people to crea...\\n\\n> Source (Doc id: aacb46d4-8fc6-4c20-84a2-0399cce3d930): URL: https://stanford-cs324.github.io/winter2022/lectures/harms-2/\\n\\ncontent, but if they can gene...\\n\\n> Source (Doc id: 8b2a5a89-459a-48ff-a82f-137bfd1b5f8b): URL: https://stanford-cs324.github.io/winter2022/lectures/harms-2/\\n\\nBalle, Atoosa Kasirzadeh, Zac...\\n\\n> Source (Doc id: 73cb8701-81b7-4d4e-bbb8-e06e8ba5b9ed): URL: https://stanford-cs324.github.io/winter2022/lectures/capabilities/\\n\\n  Capabilities | CS324  ...\\n\\n> Source (Doc id: ef478701-dd70-4793-ad99-78f16bb2775f): URL: https://stanford-cs324.github.io/winter2022/lectures/capabilities/\\n\\norsomething in between (...\\n\\n> Source (Doc id: ed1543fb-d8dc-4c9e-937e-30cea011e5cb): URL: https://stanford-cs324.github.io/winter2022/lectures/capabilities/\\n\\nwant to take the arithme...\\n\\n> Source (Doc id: a1637864-15eb-4832-a5bc-0c3e6c68da85): URL: https://stanford-cs324.github.io/winter2022/lectures/capabilities/\\n\\nentire text as a prompt ...\\n\\n> Source (Doc id: be820483-f437-4d8f-9954-50c8f1d2e6ab): URL: https://stanford-cs324.github.io/winter2022/lectures/capabilities/\\n\\nbias towards short answe...\\n\\n> Source (Doc id: 66d333b5-54f9-4670-b142-36e359307a33): URL: https://stanford-cs324.github.io/winter2022/lectures/capabilities/\\n\\ntranslate a sentence in ...\\n\\n> Source (Doc id: 6ba79880-ce38-45ef-9bfd-9706e54d5403): URL: https://stanford-cs324.github.io/winter2022/lectures/capabilities/\\n\\nBut those who opposed th...'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(str(response))\n",
    "response.get_formatted_sources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 13 tokens\n",
      "> [retrieve] Total embedding token usage: 13 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 0 tokens\n",
      "> [retrieve] Total embedding token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 2041 tokens\n",
      "> [get_response] Total LLM token usage: 2041 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 2041 tokens\n",
      "> [get_response] Total LLM token usage: 2041 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "q = \"What are some milestone model architectures and papers in the last few years\"\n",
    "response = query_engine.query(q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Some milestone model architectures and papers in the last few years include Recurrent Neural Networks (RNNs), including Long Short Term Memory (LSTMs) (2003), Transformers (2017), ELMo (2018), GPT (2018), BERT (2018), XLM (2019), GPT-2 (2019), RoBERTa (2019), Megatron-LM (2019), T5 (2019), Turing-NLG (2020), GPT-3 (2020), Megatron-Turing NLG (2020), and Gopher (2021).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'> Source (Doc id: f5fccc85-2c2e-4708-a844-1f65cf9375e0): URL: https://stanford-cs324.github.io/winter2022/lectures/introduction/\\n\\nis access. Whereas small...\\n\\n> Source (Doc id: c1270b0b-e6fe-4c7e-9073-68889202dfdb): URL: https://stanford-cs324.github.io/winter2022/lectures/introduction/\\n\\nvalues of \\\\(n\\\\).Now, the...'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(str(response))\n",
    "response.get_formatted_sources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 8 tokens\n",
      "> [retrieve] Total embedding token usage: 8 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 0 tokens\n",
      "> [retrieve] Total embedding token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1978 tokens\n",
      "> [get_response] Total LLM token usage: 1978 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1978 tokens\n",
      "> [get_response] Total LLM token usage: 1978 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "q = 'Which models did Google release in Oct 2018'\n",
    "q = \"What are the layers in a transformer block\"\n",
    "response = query_engine.query(q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The layers in a transformer block are typically a multi-head attention layer, a feed-forward layer, and a layer normalization layer.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'> Source (Doc id: 157f6610-6790-4161-aadb-b39011d58bbf): URL: https://stanford-cs324.github.io/winter2022/lectures/introduction/\\n\\n\\\\underbrace{p(\\\\text{spee...\\n\\n> Source (Doc id: c1270b0b-e6fe-4c7e-9073-68889202dfdb): URL: https://stanford-cs324.github.io/winter2022/lectures/introduction/\\n\\nvalues of \\\\(n\\\\).Now, the...'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.get_formatted_sources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 16 tokens\n",
      "> [retrieve] Total embedding token usage: 16 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 0 tokens\n",
      "> [retrieve] Total embedding token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1459 tokens\n",
      "> [get_response] Total LLM token usage: 1459 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1459 tokens\n",
      "> [get_response] Total LLM token usage: 1459 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query('Tell me about datasets used to train LLMs and how they’re cleaned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Large language models (LLMs) are trained on datasets that contain a huge amount of Internet data, such as Reddit. To ensure that the data is clean and free of bias, careful curation is necessary. Datasets such as RealToxicityPrompts are used to evaluate a language model’s propensity for producing toxic content. Additionally, datasets such as GPT-3 have been demonstrated to output anti-Muslim stereotypes. To mitigate these issues, data poisoning attacks can be used to inject poison documents into the training set. Furthermore, legal considerations must be taken into account when training language models on copyright data.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'> Source (Doc id: 58bbc077-ae7e-4fbe-915e-1caba0ffef70): URL: https://stanford-cs324.github.io/winter2022/lectures/introduction/\\n\\nfinished the program. He...\\n\\n> Source (Doc id: d06a0301-f8b0-458f-b678-63d4d477270e): URL: https://stanford-cs324.github.io/winter2022/lectures/harms-1/\\n\\nmentions 21 definitions). Unf...'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.get_formatted_sources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
